{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the code below that the data are represented as 64-bit floating point numbers, which is the default.  As the name implies, this tack requires 64 bits of memory for each floating-point value and each bit needs to be considered when multiplication is conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784, dtype('float32'))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.load(f'data/mnist1_{size}.npy').astype(np.float32)\n",
    "q = np.load(f'data/mnist2_{size}.npy').astype(np.float32)\n",
    "assert p.shape[0] == q.shape[0]\n",
    "assert p.shape[1] == 784\n",
    "assert q.shape[1] == 784\n",
    "\n",
    "n = p.shape[0]\n",
    "pixels = p.shape[1]\n",
    "n,pixels,p.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for Computing Distances\n",
    "\n",
    "The methods shown below demonstrate a range of approaches for computing the pairwise distances between MNIST images in two data sets.  The available data sets contain either 10, 100, 1000, 10000, or 60000 images, each of which is constituted by 784 pixels encoded as floating-point values.  A data set with <code>n</code> images constitutes a <code>numpy</code> that is $n \\times 784$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Python: Nested <code>for</code> Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m         sum_sq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pixels):\n\u001b[1;32m----> 7\u001b[0m             sum_sq \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (q[i][k] \u001b[38;5;241m-\u001b[39m p[j][k])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      8\u001b[0m         result[i][j] \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(sum_sq)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExec. time: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)), \u001b[38;5;28mstr\u001b[39m(n), \u001b[38;5;28mstr\u001b[39m(n)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "result = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sum_sq = 0.0\n",
    "        for k in range(pixels):\n",
    "            sum_sq += (q[i][k] - p[j][k])**2\n",
    "        result[i][j] = math.sqrt(sum_sq)\n",
    "\n",
    "print('Exec. time: %s for %sx%s' % (str(float(time.time() - start)), str(n), str(n)))\n",
    "print(result[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Squared Difference with Matrix-Vector Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec. time: 7.344522476196289 for 1000x1000\n",
      "[10.553867 10.057242  8.83383   9.928222  8.737532]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "result = np.zeros((n,n)).astype(np.float32)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        result[i][j] = np.sqrt(q[i]@q[i]-2*p[j]@q[i]+p[j]@p[j])\n",
    "\n",
    "print('Exec. time: %s for %sx%s' % (str(float(time.time() - start)), str(n), str(n)))\n",
    "print(result[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise (Vectorized) Math with <code>numpy</code> and Nested <code>for</code> Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec. time: 4.896218776702881 for 1000x1000\n",
      "[10.553867 10.057242  8.83383   9.928222  8.737532]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "result = np.zeros((n,n)).astype(np.float32)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        result[i][j] = np.sqrt(((q[i]-p[j])**2).sum())\n",
    "\n",
    "print('Exec. time: %s for %sx%s' % (str(float(time.time() - start)), str(n), str(n)))\n",
    "print(result[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <code>numpy np.newaxis()</code> with Broadcasting to Avoid Loops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec. time: 3.5109074115753174 for 1000x1000\n",
      "[10.553867 10.057242  8.83383   9.928222  8.737532]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#result = np.sqrt((q-p)@(q-p).T)\n",
    "result = np.sqrt(((q[:, np.newaxis, :] - p)*(q[:, np.newaxis, :] - p)).sum(axis=2))\n",
    "\n",
    "print('Exec. time: %s for %sx%s' % (str(float(time.time() - start)), str(n), str(n)))\n",
    "print(result[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the <code>numpy</code> Method <code> np.linalg.norm()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec. time: 2.414586305618286 for 1000x1000\n",
      "[10.553867 10.057242  8.83383   9.928222  8.737532]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#result = np.sqrt((q-p)@(q-p).T)\n",
    "result = np.linalg.norm(q[:, np.newaxis, :] - p, axis=2)\n",
    "\n",
    "print('Exec. time: %s for %sx%s' % (str(float(time.time() - start)), str(n), str(n)))\n",
    "print(result[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the <code>scipy</code> Method <code> scipy.spatial.distance.cdist()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec. time: 0.46059441566467285 for 1000x1000\n",
      "[10.5538673  10.05724184  8.83382935  9.92822226  8.73753215]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "start = time.time()\n",
    "result = cdist(q,p)\n",
    "print('Exec. time: %s for %sx%s' % (str(float(time.time() - start)), str(n), str(n)))\n",
    "print(result[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to <code>np.einsum()</code>\n",
    "\n",
    "This <code>numpy</code> quickly multiplies arrays while summing across specified axis.  It requires as input a string in \"Einstein Notation\" that determines how the arrays will be multipled (e.g., vector-matrix multiplcation versus elementwise (vectorized) multiplication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstein Notation\n",
    "\n",
    "Einstein notation give a roadmap for how <code>numpy</code> arrays and vectors are to be multiplied using single-characters each of which corresponds to a dimension (think rows and columns) of a <code>numpy</code> that is to be multiplied.  Further, when <code>np.einsum()</code> is used sums are often computed on the array that results from the specified multiplication operation.  \n",
    "\n",
    "A typical example of Einstein notation for multiplying two 2D arrays is:\n",
    "\n",
    "``` python\n",
    "'ij,jk->ik'\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    " - <code>i</code> represents the rows of the first input and <code>j</code> its column indices\n",
    " - <code>j</code> represents the rows of the second input and <code>k</code> its column indices\n",
    " - The commonality of <code>j</code> indicates that the first array has the same number of columns as the second array has rows.  (Essential for matrix math.)\n",
    " - The symbol <code>-></code> separates inputs on the left from the output on the right\n",
    " - <code>ik</code> indicates that the output array has rows corresponding with the first input and columns corresponding with the second input and, further, that the elements multiplied along <code>j</code> are to be summed.\n",
    "\n",
    "As is demonstrated below, these two characteristics of the letters/indices used are important:\n",
    "\n",
    "- The order of the indices in the Einstein notation\n",
    "- Which indices used to represent original inputs are included in the specification of the output, or not\n",
    "- Where input index parameters are not represented in the output, it indicates where summing is to take place.\n",
    "\n",
    "The following page documents Einstien notation and gives examples: [link_fort_Einstein_Notation](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)\n",
    "\n",
    "Let's demonstrate with two small arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [0 4]] \n",
      "\n",
      " [[2 3]\n",
      " [4 0]]\n"
     ]
    }
   ],
   "source": [
    "mat1 = np.random.randint(0,5,(2,2))\n",
    "mat2 = np.random.randint(0,5,(2,2))\n",
    "print(mat1,'\\n\\n',mat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first example does not use <code>np.einsum()</code> but it, rather, shows three calculations that we might make with the two arrays, respectively, \n",
    "\n",
    "- The dot product of the two arrays \n",
    "- Elementwise multiplication of the two arrays\n",
    "- The dot product of the two arays, subsequently summed along axis 0\n",
    "- The dot product of the two arays, subsequently summed along axis 1\n",
    "- The dot product of the two arays, subsequently summed along both axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product\n",
      " [[10  9]\n",
      " [16  0]]\n",
      "\n",
      "Dot Product, summed on axis=0\n",
      " [26  9]\n",
      "\n",
      "Dot Product, summed on axis=1\n",
      " [19 16]\n",
      "\n",
      "Dot Product, summed on both axes\n",
      " 35\n",
      "\n",
      "Elementwise Product\n",
      " [[6 3]\n",
      " [0 0]]\n",
      "\n",
      "Elementwise Product, summed on axis=0\n",
      " [6 3]\n",
      "\n",
      "Elementwise Product, summed on axis=1\n",
      " [9 0]\n",
      "\n",
      "Elementwise Product, summed on both axes\n",
      " 9\n"
     ]
    }
   ],
   "source": [
    "''' Dot products/matrix-vector math mode '''\n",
    "print('Dot Product\\n',mat1@mat2)\n",
    "print('\\nDot Product, summed on axis=0\\n',(mat1@mat2).sum(axis=0))\n",
    "print('\\nDot Product, summed on axis=1\\n',(mat1@mat2).sum(axis=1))\n",
    "print('\\nDot Product, summed on both axes\\n',(mat1@mat2).sum())\n",
    "\n",
    "''' Elementwise/vectorized computations '''\n",
    "print('\\nElementwise Product\\n', mat1*mat2)\n",
    "print('\\nElementwise Product, summed on axis=0\\n',(mat1*mat2).sum(axis=0))\n",
    "print('\\nElementwise Product, summed on axis=1\\n',(mat1*mat2).sum(axis=1))\n",
    "print('\\nElementwise Product, summed on both axes\\n',(mat1*mat2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first example of <code>np.einsum()</code> uses Einstein notation that denotes the dot product:\n",
    "\n",
    "``` python\n",
    "'ij,jk->ik'\n",
    "```\n",
    "\n",
    "Notably, the <code>j</code> relates to the column indices of the first input as well as the row indices of the second input, which indicates that the elements in the rows of the first input are to be multiplied by the columns in the second input, as is done with the dot product.  The <code>ik</code> indicates that the result has two dimensions with the row indices relating to the first input and the column indices relating to the second input, again, as is the case with the dot product.\n",
    "\n",
    "Note that the absence of an output specification results in a dot product with the input specifications as shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  9]\n",
      " [16  0]]\n"
     ]
    }
   ],
   "source": [
    "result = np.einsum('ij,jk->ik', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  9]\n",
      " [16  0]]\n"
     ]
    }
   ],
   "source": [
    "result = np.einsum('ij,jk', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was claimed earlier that the order of the indices mattered, as is demonstrated below.  reversing the <code>i</code> and <code>k</code> in the output specification causes the output to be the transpose of the prior respective result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 16]\n",
      " [ 9  0]]\n"
     ]
    }
   ],
   "source": [
    "result = np.einsum('ij,jk->ki', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two computations below sum along either of the axes depending whether the output is speciified to be related with the <code>i</code> or <code>j</code> indices.  <code>i</code> refers to the rows of the first input, and so the sum is along the columns.  Conversely, specifying the output with <code>k</code> sums along axis 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26  9]\n"
     ]
    }
   ],
   "source": [
    "result = np.einsum('ij,jk->k', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 16]\n"
     ]
    }
   ],
   "source": [
    "result = np.einsum('ij,jk->i', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing along both axes with dot product being indicated with the inputs requires the <code>-></code> symbol with no output specification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "result = np.einsum('ij,jk->', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementwise multiplication, without summing, is down by listing the indices characters in the same order, with both indices characters also listed in the output specification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 3]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "result = np.einsum('ij,ij->ij', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to a previous example, one can generate the transpose result by switching the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.einsum('ij,ij->ji', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing along axes is done as shown before with dot products.  Listing <code>i</code> will cause the summation to occur across the unlisted indices (<code>j</code>), that is across the columns as in <code>axis=1</code>.  Conversely listing <code>j</code> in the output specification causes summation as in <code>axis=1</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.einsum('ij,ij->i', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.einsum('ij,ij->j', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing along all axes in accomplishing by omitting an output specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.einsum('ij,ij', mat1, mat2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einstein notation is just as applicable with one dimensional vectors (single dimension arrays) but they are, of course, represented with only one index character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_one = np.ones(mat1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.einsum('ij,j',mat1,vec_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.einsum('ij,i',mat1,vec_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.einsum('ij,i->j',mat1,vec_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.einsum('i,ij',vec_one,mat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.einsum('ij,i->',mat1,vec_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Distances with <code>np.einsum()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# for i in range(n):\n",
    "#    for j in range(n):\n",
    "#        result[i][j] = np.sqrt(q[i]@q[i]-2*p[j]@q[i]+p[j]@p[j])\n",
    "\n",
    "result_ein = np.sqrt(np.einsum('ij,ij->i',q,q)[:,np.newaxis] - 2*q@p.T + np.einsum('ij,ij->i',p,p))\n",
    "\n",
    "print('Exec. time: %s for %sx%s' % (str(float(time.time() - start)), str(n), str(n)))\n",
    "print(result_ein[0,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.absolute(result - result_ein).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
